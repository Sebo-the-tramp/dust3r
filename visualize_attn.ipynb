{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from dust3r.inference import inference\n",
    "from dust3r.model import AsymmetricCroCo3DStereo\n",
    "from dust3r.utils.image import load_images\n",
    "from dust3r.image_pairs import make_pairs\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "batch_size = 1\n",
    "schedule = 'cosine'\n",
    "lr = 0.01\n",
    "niter = 300\n",
    "\n",
    "model_name = \"naver/DUSt3R_ViTLarge_BaseDecoder_512_dpt\"\n",
    "# you can put the path to a local checkpoint in model_name if needed\n",
    "model = AsymmetricCroCo3DStereo.from_pretrained(model_name).to(device)\n",
    "\n",
    "# a dict to store the activations\n",
    "# trying to add hooks on the attention layers    \n",
    "activation = {}\n",
    "def getActivation(name):\n",
    "    # the hook signature\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# add hooks to the model\n",
    "proj_q_1 = model.dec_blocks[0].cross_attn.register_forward_hook(getActivation('projq_1'))\n",
    "proj_k_1 = model.dec_blocks[0].cross_attn.register_forward_hook(getActivation('projk_1'))\n",
    "\n",
    "proj_q_2 = model.dec_blocks2[0].cross_attn.register_forward_hook(getActivation('projq_2'))\n",
    "proj_k_2 = model.dec_blocks2[0].cross_attn.register_forward_hook(getActivation('projk_2'))\n",
    "\n",
    "# load_images can take a list of images or a directory\n",
    "images = load_images(['croco/assets/Chateau1.png', 'croco/assets/Chateau2.png'], size=512)\n",
    "pairs = make_pairs(images, scene_graph='complete', prefilter=None, symmetrize=True)\n",
    "output = inference(pairs, model, device, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "#collection of the activations\n",
    "projq_1 = activation['projq_1']\n",
    "projk_1 = activation['projk_1']\n",
    "\n",
    "projq_2 = activation['projq_2']\n",
    "projk_2 = activation['projk_2']\n",
    "\n",
    "print(projq_1.shape)  # Should print [1, 768, 768]\n",
    "print(projk_1.shape)  # Should print [1, 768, 768]\n",
    "\n",
    "B, N, C = projq_1.shape  # B=1, N=768, C=768\n",
    "num_heads = 8\n",
    "head_dim = C // num_heads  # 96\n",
    "\n",
    "projq_1 = projq_1.view(B, N, num_heads, head_dim).transpose(1, 2)  # [1, 8, 768, 96]\n",
    "projk_1 = projk_1.view(B, N, num_heads, head_dim).transpose(1, 2)  # [1, 8, 768, 96]\n",
    "\n",
    "projq_2 = projq_2.view(B, N, num_heads, head_dim).transpose(1, 2)  # [1, 8, 768, 96]\n",
    "projk_2 = projk_2.view(B, N, num_heads, head_dim).transpose(1, 2)  # [1, 8, 768, 96]\n",
    "\n",
    "# Compute attention scores\n",
    "attn_scores_1 = torch.matmul(projq_1, projk_1.transpose(-2, -1))  # [1, 8, 768, 768]\n",
    "attn_scores_2 = torch.matmul(projq_2, projk_2.transpose(-2, -1))  # [1, 8, 768, 768]\n",
    "\n",
    "# Scale the scores\n",
    "attn_scores_1 = attn_scores_1 / math.sqrt(head_dim)  # [1, 8, 768, 768]\n",
    "attn_scores_2 = attn_scores_2 / math.sqrt(head_dim)  # [1, 8, 768, 768]\n",
    "\n",
    "# Apply softmax to get attention weights\n",
    "attention_weights_1 = torch.softmax(attn_scores_1, dim=-1)  # [1, 8, 768, 768]\n",
    "attention_weights_2 = torch.softmax(attn_scores_2, dim=-1)  # [1, 8, 768, 768]\n",
    "\n",
    "print(attention_weights_1.shape)  # Should print [1, 8, 768, 768]\n",
    "print(attention_weights_2.shape)  # Should print [1, 8, 768, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for head_idx in range(8):\n",
    "\n",
    "    # Extract attention matrix for the selected head\n",
    "    attention_matrix = attention_weights_1[0, head_idx].detach().cpu().numpy()  # [768, 768]\n",
    "\n",
    "    # Verify number of patches\n",
    "    num_patches = 768\n",
    "    grid_height = 32\n",
    "    grid_width = 24\n",
    "\n",
    "    assert grid_height * grid_width == num_patches, \"Grid dimensions do not match number of patches.\"\n",
    "\n",
    "    # Reshape to [32, 24, 32, 24]\n",
    "    attention_grid = attention_matrix.reshape(grid_height, grid_width, grid_height, grid_width)  # [32, 24, 32, 24]\n",
    "\n",
    "    # Aggregate attention across query patches\n",
    "    aggregated_attention = attention_grid.sum(axis=(0, 1))  # [32, 24]\n",
    "\n",
    "    # Normalize the attention map\n",
    "    aggregated_attention = aggregated_attention / aggregated_attention.max()\n",
    "\n",
    "    # print(aggregated_attention.shape)  # Should print (32, 24)\n",
    "\n",
    "    # Load your image\n",
    "    image_path = 'croco/assets/Chateau1.png'  # Replace with your actual image path\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_np = np.array(image)\n",
    "\n",
    "    image_path_other_view = 'croco/assets/Chateau2.png'  # Replace with your actual image path\n",
    "    image_other_view = Image.open(image_path_other_view).convert('RGB')\n",
    "    image_np_other_view = np.array(image_other_view)\n",
    "\n",
    "    # Confirm image dimensions\n",
    "    # print(image_np.shape)  # Should print (512, 384, 3)\n",
    "\n",
    "    # Resize the aggregated attention map to match the image size\n",
    "    attention_map = aggregated_attention  # [32, 24]\n",
    "\n",
    "    # Use OpenCV to resize\n",
    "    attention_map_resized = cv2.resize(attention_map, (image_np.shape[1], image_np.shape[0]))  # (384, 512)\n",
    "\n",
    "    # Normalize the attention map to [0, 255]\n",
    "    attention_map_resized = np.uint8(255 * attention_map_resized)\n",
    "\n",
    "    # Apply a color map (e.g., JET)\n",
    "    attention_map_colored = cv2.applyColorMap(attention_map_resized, cv2.COLORMAP_JET)\n",
    "\n",
    "    # Convert from BGR (OpenCV default) to RGB\n",
    "    attention_map_colored = cv2.cvtColor(attention_map_colored, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Blend the attention map with the original image\n",
    "    alpha = 0.6  # Transparency factor for the original image\n",
    "    beta = 0.4   # Transparency factor for the attention map\n",
    "    gamma = 0    # Scalar added to each sum\n",
    "\n",
    "    overlayed_image = cv2.addWeighted(image_np, alpha, attention_map_colored, beta, gamma)\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(18, 6))\n",
    "\n",
    "    # Original Image\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.imshow(image_np_other_view)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Aggregated Attention Heatmap\n",
    "    plt.subplot(1, 4, 2)\n",
    "    sns.heatmap(aggregated_attention, cmap='viridis')\n",
    "    plt.title(f'Aggregated Attention - Head {head_idx + 1}')\n",
    "    plt.xlabel('Key Patch X (Width)')\n",
    "    plt.ylabel('Key Patch Y (Height)')\n",
    "\n",
    "    # Overlayed Image\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.imshow(overlayed_image)\n",
    "    plt.title('Overlayed Attention Map')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Other image Image\n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for head_idx in range(8):\n",
    "\n",
    "    # Extract attention matrix for the selected head\n",
    "    attention_matrix = attention_weights_2[0, head_idx].detach().cpu().numpy()  # [768, 768]\n",
    "\n",
    "    # print(attention_matrix.shape)  # Should print (768, 768)\n",
    "\n",
    "    # Verify number of patches\n",
    "    num_patches = 768\n",
    "    grid_height = 32\n",
    "    grid_width = 24\n",
    "\n",
    "    assert grid_height * grid_width == num_patches, \"Grid dimensions do not match number of patches.\"\n",
    "\n",
    "    # Reshape to [32, 24, 32, 24]\n",
    "    attention_grid = attention_matrix.reshape(grid_height, grid_width, grid_height, grid_width)  # [32, 24, 32, 24]\n",
    "\n",
    "    # Aggregate attention across query patches\n",
    "    aggregated_attention = attention_grid.sum(axis=(0, 1))  # [32, 24]\n",
    "\n",
    "    # Normalize the attention map\n",
    "    aggregated_attention = aggregated_attention / aggregated_attention.max()\n",
    "\n",
    "    # print(aggregated_attention.shape)  # Should print (32, 24)\n",
    "\n",
    "    # Load your image\n",
    "    image_path = 'croco/assets/Chateau2.png'  # Replace with your actual image path\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_np = np.array(image)\n",
    "\n",
    "    image_path_other_view = 'croco/assets/Chateau1.png'  # Replace with your actual image path\n",
    "    image_other_view = Image.open(image_path_other_view).convert('RGB')\n",
    "    image_np_other_view = np.array(image_other_view)\n",
    "\n",
    "    # Confirm image dimensions\n",
    "    # print(image_np.shape)  # Should print (512, 384, 3)\n",
    "\n",
    "    # Resize the aggregated attention map to match the image size\n",
    "    attention_map = aggregated_attention  # [32, 24]\n",
    "\n",
    "    # Use OpenCV to resize\n",
    "    attention_map_resized = cv2.resize(attention_map, (image_np.shape[1], image_np.shape[0]))  # (384, 512)\n",
    "\n",
    "    # Normalize the attention map to [0, 255]\n",
    "    attention_map_resized = np.uint8(255 * attention_map_resized)\n",
    "\n",
    "    # Apply a color map (e.g., JET)\n",
    "    attention_map_colored = cv2.applyColorMap(attention_map_resized, cv2.COLORMAP_JET)\n",
    "\n",
    "    # Convert from BGR (OpenCV default) to RGB\n",
    "    attention_map_colored = cv2.cvtColor(attention_map_colored, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Blend the attention map with the original image\n",
    "    alpha = 0.6  # Transparency factor for the original image\n",
    "    beta = 0.4   # Transparency factor for the attention map\n",
    "    gamma = 0    # Scalar added to each sum\n",
    "\n",
    "    overlayed_image = cv2.addWeighted(image_np, alpha, attention_map_colored, beta, gamma)\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(18, 6))\n",
    "\n",
    "    # Original Image\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.imshow(image_np_other_view)\n",
    "    plt.title('Source Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Aggregated Attention Heatmap\n",
    "    plt.subplot(1, 4, 2)\n",
    "    sns.heatmap(aggregated_attention, cmap='viridis')\n",
    "    plt.title(f'Aggregated Attention - Head {head_idx + 1}')\n",
    "    plt.xlabel('Key Patch X (Width)')\n",
    "    plt.ylabel('Key Patch Y (Height)')\n",
    "\n",
    "    # Overlayed Image\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.imshow(overlayed_image)\n",
    "    plt.title('Target Overlayed Attention Map')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Other image Image\n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Target Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
